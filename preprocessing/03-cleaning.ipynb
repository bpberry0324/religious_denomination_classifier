{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "793da3fb-00c4-48a4-8ae7-263e0cf8f467",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1101dc9f-68dc-470e-bb0f-e2c91d4fd43e",
   "metadata": {},
   "source": [
    "For a text-heavy classification problem, a lot of parsing is necessary up front. The approach taken was to create a data frame that consisted of the combined text of each post (the title and the post body) in a handful of different ways, differing slightly in the methods used to process/tokenize them. \n",
    "\n",
    "Initial imports called for text-cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61fe21d3-7b9e-42a3-b675-c0a5d2013898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c686a26f-361e-48ca-972a-2e29ecc4ad4c",
   "metadata": {},
   "source": [
    "The initial four datasets collected from PushShift are read in and combined below to form what becomes our main dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ded5ad1-f398-474f-a6ee-aea5fd0dc851",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv('../datasets/Catholicism.csv'), \n",
    "                pd.read_csv('../datasets/Catholicism2.csv'), \n",
    "                pd.read_csv('../datasets/OrthodoxChristianity.csv'), \n",
    "                pd.read_csv('../datasets/OrthodoxChristianity2.csv')]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781fcd57-ff32-45cb-b593-95270627aa71",
   "metadata": {},
   "source": [
    "Null values are deleted, as are any entries where posts were deleted or removed by an administrator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd918a41-0d5b-4be5-9da1-68a5ca88ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['selftext'].notnull()]\n",
    "df = df[df['selftext'] != '[removed]']\n",
    "df = df[df['selftext'] != '[deleted]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c60ac-6378-4e8e-adc1-1ae376813fd2",
   "metadata": {},
   "source": [
    "Title and post body are combined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6345687-495c-4238-a790-08ea66cfc32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['alltext'] = df['title'] + ' ' + df['selftext']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cae686-fffd-4c56-a1ee-99049059e56c",
   "metadata": {},
   "source": [
    "Here the new combined text feature is run through a laundry list of string parsing functions to either remove unintelligible markdown or convert it to something meaningful where possible. URLs are also removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76894853-fc13-41b1-a088-afd9d79ca921",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['alltext'] = df['alltext'].str.replace('\\n', ' ')\n",
    "df['alltext'] = df['alltext'].str.replace('\\t', ' ')\n",
    "df['alltext'] = df['alltext'].str.replace('\\r', ' ')\n",
    "df['alltext'] = df['alltext'].str.replace(\"\\'\", \"\")\n",
    "df['alltext'] = df['alltext'].str.replace('\"', ' ')\n",
    "df['alltext'] = df['alltext'].str.replace('&gt', '>')\n",
    "df['alltext'] = df['alltext'].str.replace('&ge', '>=')\n",
    "df['alltext'] = df['alltext'].str.replace('&lt', '<')\n",
    "df['alltext'] = df['alltext'].str.replace('&le', '<=')\n",
    "df['alltext'] = df['alltext'].map(lambda x: re.sub(r\"http\\S+\", \"\", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c60b88f-9350-40fe-9064-02c8d8f0c063",
   "metadata": {},
   "source": [
    "Text is converted to lowercase and fed into a new column with stopwords removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73bac4a6-8341-42fb-9395-3e11489bd5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lowtext'] = df['alltext'].str.lower()\n",
    "df['unstopped'] = df['lowtext'].map(lambda x: ' '.join([word for word in x.split() if word not in stopwords.words('english')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626ca38e-9194-4758-a4ff-4e188d70b5e1",
   "metadata": {},
   "source": [
    "Below, two tokenizing objects are created, as are a lemmatization object and a stemming object respectively, to aid in the preprocessing that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34037c2c-0c5a-48ed-ac56-531cba27847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok1 = RegexpTokenizer(r'\\w+')\n",
    "tok2 = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6607c4f-dcf2-48e4-a092-0325459a24cf",
   "metadata": {},
   "source": [
    "Here, four columns are created, each with a slightly different version of the entry's text. This was done in the beginning to facilitate the tracking of nuances in the model results from as early on as possible. The versions are:\n",
    "\n",
    "- the words of the combined title and post body, lemmatized\n",
    "- the words of the combined title and post body, stemmed\n",
    "- the words and potentially significant numbers of the combined title and post body, lemmatized\n",
    "- the words and potentially significant numbers of the combined title and post body, stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "679fa1d6-3a34-4707-a553-5444a6579aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['simp_lem'] = df['unstopped'].map(lambda x: ' '.join(tok1.tokenize(x)))\n",
    "df['simp_lem'] = df['simp_lem'].map(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "df['simp_stem'] = df['unstopped'].map(lambda x: ' '.join(tok1.tokenize(x)))\n",
    "df['simp_stem'] = df['simp_stem'].map(lambda x: ' '.join([p_stemmer.stem(word) for word in x.split()]))\n",
    "\n",
    "df['comp_lem'] = df['unstopped'].map(lambda x: ' '.join(tok2.tokenize(x)))\n",
    "df['comp_lem'] = df['comp_lem'].map(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "df['comp_stem'] = df['unstopped'].map(lambda x: ' '.join(tok2.tokenize(x)))\n",
    "df['comp_stem'] = df['comp_stem'].map(lambda x: ' '.join([p_stemmer.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab00b2f9-2e8e-42a0-8c83-0be9c9a81bbe",
   "metadata": {},
   "source": [
    "Target feature is binarized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09203267-880c-424b-aa53-e4c5bbe5fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subreddit'] = df['subreddit'].map({'Catholicism' : 1, 'OrthodoxChristianity' : 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c76c6f-a527-4d9f-ab81-4b0cae976c8a",
   "metadata": {},
   "source": [
    "The result is written to its `.csv` file in the `datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5ebe71b-454b-4cc5-8bac-9927bcdc55b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../datasets/main.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
